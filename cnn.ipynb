{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando uma CNN para reconhecimento de imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_O objetivo desse notebook é implementar uma rede neural convolucional que tem como função reconhecer se a imagem inputada é de um cachorro ou de um gato._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grupo: Daniel Augusto, Douglas Abdo, Matheus Fialho e Rennan Haro.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ferramentas utilizadas: [TensorFlow](https://www.tensorflow.org), [Keras](https://keras.io). <br>\n",
    "**Para rodar a rede neural em sua máquina, instale o [Python (3.6+)](https://python.org/download) ou [Anaconda](https://www.anaconda.com/products/individual) e execute o comando `$ pip install -r requirements.txt`** <br>\n",
    "Desenvolvido e implementado por [Rennan Haro](https://linkedin.com/in/rennanharo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps para implementaçao da CNN\n",
    "- Importar as bibliotecas necessárias\n",
    "- Coletar o dataset\n",
    "- Separar o dataset (split)\n",
    "- Setar as varáveis para a construção da CNN\n",
    "- Estruturar a CNN\n",
    "- Data augmentation\n",
    "- Treinar a nossa rede neural\n",
    "- Testar nosso algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante: execute o comando `$ pip install -r requirements.txt` antes de prosseguir com esse step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os modulos do Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os modulos adicionais\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 e 3 - Coletando e splitando o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Encontre mais informações sobre o dataset e downloads adicionais [neste link](https://www.kaggle.com/c/dogs-vs-cats/data)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os datasets estão na pasta `data`. Consistem em basicamente 4 datasets (2 para treinar e 2 para testar), totalizando 2.800 imagens. 1.400 imagens de gatos e 1.400 imagens de cachorros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O split do dataset já foi feito. <br>\n",
    "Temos em nosso dataset de treino 1.000 imagens de cachorros e 1.000 imagens de gatos. <br>\n",
    "O nosso datset de teste, por sua vez, possui 400 imagens de cada.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imagem](assets/kaggle_dataset.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Setando variáveis (hiperparâmetros) para construção da nossa CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150 # Dimensão das imagens\n",
    "\n",
    "train_data_dir = 'data/train' # Diretório com dados para treino\n",
    "validation_data_dir = 'data/validation' # Diretório com dados para teste\n",
    "\n",
    "nb_train_samples = 2000 # Quantidade de samples para treino\n",
    "nb_validation_samples = 800 # Quantidade de samples para teste\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# Setando o shape das imagens inputadas\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Construindo a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Treinando nossa CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-240a007c2074>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.7180 - accuracy: 0.5180 - val_loss: 0.6851 - val_accuracy: 0.6425\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.6729 - accuracy: 0.5940 - val_loss: 0.6230 - val_accuracy: 0.6575\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.6408 - accuracy: 0.6505 - val_loss: 0.6077 - val_accuracy: 0.6500\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 36s 285ms/step - loss: 0.6114 - accuracy: 0.6715 - val_loss: 0.5810 - val_accuracy: 0.6750\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.5887 - accuracy: 0.6935 - val_loss: 0.6071 - val_accuracy: 0.6925\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.5795 - accuracy: 0.7065 - val_loss: 0.6443 - val_accuracy: 0.6787\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 36s 288ms/step - loss: 0.5529 - accuracy: 0.7240 - val_loss: 0.5337 - val_accuracy: 0.7250\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 36s 287ms/step - loss: 0.5527 - accuracy: 0.7205 - val_loss: 0.5221 - val_accuracy: 0.7400\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 36s 291ms/step - loss: 0.5390 - accuracy: 0.7445 - val_loss: 0.5499 - val_accuracy: 0.7350\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.5106 - accuracy: 0.7625 - val_loss: 0.5252 - val_accuracy: 0.7262\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 36s 289ms/step - loss: 0.4986 - accuracy: 0.7700 - val_loss: 0.5471 - val_accuracy: 0.7138\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.4871 - accuracy: 0.7825 - val_loss: 0.4981 - val_accuracy: 0.7625\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.4920 - accuracy: 0.7765 - val_loss: 0.5182 - val_accuracy: 0.7475\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.4785 - accuracy: 0.7790 - val_loss: 0.4984 - val_accuracy: 0.7812\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.4840 - accuracy: 0.7770 - val_loss: 0.5255 - val_accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 38s 303ms/step - loss: 0.4783 - accuracy: 0.7830 - val_loss: 0.5309 - val_accuracy: 0.7462\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 38s 306ms/step - loss: 0.4556 - accuracy: 0.7835 - val_loss: 0.5691 - val_accuracy: 0.7600\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 38s 306ms/step - loss: 0.4367 - accuracy: 0.8090 - val_loss: 0.5241 - val_accuracy: 0.7862\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.4411 - accuracy: 0.8025 - val_loss: 0.5663 - val_accuracy: 0.7800\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 39s 314ms/step - loss: 0.4389 - accuracy: 0.8015 - val_loss: 0.4835 - val_accuracy: 0.7763\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 39s 313ms/step - loss: 0.4297 - accuracy: 0.8145 - val_loss: 0.5211 - val_accuracy: 0.7638\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 40s 316ms/step - loss: 0.4133 - accuracy: 0.8155 - val_loss: 0.5274 - val_accuracy: 0.7763\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 41s 331ms/step - loss: 0.4147 - accuracy: 0.8160 - val_loss: 0.5282 - val_accuracy: 0.7763\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 60s 480ms/step - loss: 0.4125 - accuracy: 0.8220 - val_loss: 0.5209 - val_accuracy: 0.7563\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 53s 420ms/step - loss: 0.3959 - accuracy: 0.8345 - val_loss: 0.5034 - val_accuracy: 0.7663\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 45s 361ms/step - loss: 0.4084 - accuracy: 0.8235 - val_loss: 0.4922 - val_accuracy: 0.7975\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 59s 470ms/step - loss: 0.3900 - accuracy: 0.8320 - val_loss: 0.5437 - val_accuracy: 0.7663\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 48s 381ms/step - loss: 0.3896 - accuracy: 0.8425 - val_loss: 0.4868 - val_accuracy: 0.7837\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 46s 367ms/step - loss: 0.3967 - accuracy: 0.8350 - val_loss: 0.7098 - val_accuracy: 0.7663\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 41s 332ms/step - loss: 0.3771 - accuracy: 0.8275 - val_loss: 0.8917 - val_accuracy: 0.7462\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 0.3853 - accuracy: 0.8380 - val_loss: 0.6078 - val_accuracy: 0.7912\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 57s 457ms/step - loss: 0.3838 - accuracy: 0.8335 - val_loss: 0.5940 - val_accuracy: 0.7713\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.3855 - accuracy: 0.8465 - val_loss: 0.6979 - val_accuracy: 0.8100\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 49s 392ms/step - loss: 0.4011 - accuracy: 0.8360 - val_loss: 0.5581 - val_accuracy: 0.7862\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 48s 384ms/step - loss: 0.3855 - accuracy: 0.8475 - val_loss: 0.7092 - val_accuracy: 0.7850\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 45s 363ms/step - loss: 0.3933 - accuracy: 0.8445 - val_loss: 0.6172 - val_accuracy: 0.7638\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 45s 363ms/step - loss: 0.3841 - accuracy: 0.8355 - val_loss: 0.5449 - val_accuracy: 0.7887\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 43s 342ms/step - loss: 0.3833 - accuracy: 0.8380 - val_loss: 0.5183 - val_accuracy: 0.7887\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 43s 342ms/step - loss: 0.3802 - accuracy: 0.8525 - val_loss: 0.5069 - val_accuracy: 0.7850\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 44s 350ms/step - loss: 0.3863 - accuracy: 0.8375 - val_loss: 0.5827 - val_accuracy: 0.7713\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 43s 342ms/step - loss: 0.3697 - accuracy: 0.8485 - val_loss: 0.5885 - val_accuracy: 0.7575\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 43s 343ms/step - loss: 0.3921 - accuracy: 0.8425 - val_loss: 0.6642 - val_accuracy: 0.7962\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 43s 342ms/step - loss: 0.3708 - accuracy: 0.8530 - val_loss: 0.6267 - val_accuracy: 0.7750\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 42s 340ms/step - loss: 0.3940 - accuracy: 0.8405 - val_loss: 0.7532 - val_accuracy: 0.7812\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 43s 341ms/step - loss: 0.4082 - accuracy: 0.8340 - val_loss: 0.6056 - val_accuracy: 0.8012\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 43s 343ms/step - loss: 0.3917 - accuracy: 0.8525 - val_loss: 0.5911 - val_accuracy: 0.7875\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 43s 342ms/step - loss: 0.3695 - accuracy: 0.8545 - val_loss: 0.5665 - val_accuracy: 0.7725\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 43s 341ms/step - loss: 0.3794 - accuracy: 0.8495 - val_loss: 0.5354 - val_accuracy: 0.7887\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 43s 345ms/step - loss: 0.3785 - accuracy: 0.8455 - val_loss: 0.6873 - val_accuracy: 0.8062\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 43s 344ms/step - loss: 0.3698 - accuracy: 0.8480 - val_loss: 0.6803 - val_accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Testando nossa CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('random.png', target_size = (150, 150))\n",
    "#test_image = image.load_img('data/validation/dogs/dog.10500.jpg', target_size = (150, 150))\n",
    "#test_image = image.load_img('data/validation/cats/cat.1000.jpg', target_size = (150, 150))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = test_image/.255\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "train_generator.class_indices\n",
    "if result[0][0] >= 0.5:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "    \n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
